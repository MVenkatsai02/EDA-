{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1 Loading differnet types of files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv('filename.csv')\n",
    "# Load Excel file\n",
    "df = pd.read_excel('filename.xlsx') \n",
    "# Load JSON file\n",
    "df = pd.read_json('filename.json')\n",
    "# Load HTML file\n",
    "df_list = pd.read_html('filename.html') \n",
    "# Load Parquet file\n",
    "df = pd.read_parquet('filename.parquet')\n",
    "# Load Feather file\n",
    "df = pd.read_feather('filename.feather')\n",
    "# Load Pickle file\n",
    "df = pd.read_pickle('filename.pkl')\n",
    "# Load Stata file\n",
    "df = pd.read_stata('filename.dta')\n",
    "# Load SPSS file\n",
    "df = pd.read_spss('filename.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the some of the methods to load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2 Checking/Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()  # First few rows\n",
    "df.tail() # Last few rows\n",
    "df.info()  # Data types and non-null values\n",
    "df.describe()  # Summary statistics\n",
    "df.shape  # Dimensions of the dataset\n",
    "df.dtypes # Checking data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the some steps to analyze the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3 Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "# Drop rows with missing values\n",
    "data_cleaned = data.dropna()\n",
    "\n",
    "# Fill missing values with the mean (for numerical columns)\n",
    "data_filled = data.fillna(data.mean())\n",
    "\n",
    "df.fillna(df.mean(), inplace=True)  # For numerical columns\n",
    "df.fillna('Unknown', inplace=True)  # For categorical columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and remove duplicate rows\n",
    "data_cleaned = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize potential outliers using a box plot\n",
    "plt.boxplot(data['numerical_column'])\n",
    "plt.show()\n",
    "\n",
    "# Remove outliers (example using Z-score method)\n",
    "from scipy import stats\n",
    "data = data[(np.abs(stats.zscore(data['numerical_column'])) < 3)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the some steps to clean the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4 Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature (example: total sales = price * quantity)\n",
    "data['total_sales'] = data['price'] * data['quantity']\n",
    "\n",
    "# Polynomial Features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Interaction Features\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "interaction = FunctionTransformer(lambda X: X[:, 0] * X[:, 1], validate=True)\n",
    "interaction_pipe = Pipeline([('interaction', interaction)])\n",
    "X_interaction = interaction_pipe.fit_transform(X)\n",
    "\n",
    "# Binning\n",
    "df['binned_column'] = pd.cut(df['numerical_column'], bins=5, labels=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "data[['numerical_column1', 'numerical_column2']] = scaler.fit_transform(data[['numerical_column1', 'numerical_column2']])\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = MinMaxScaler()\n",
    "data[['numerical_column1', 'numerical_column2']] = scaler.fit_transform(data[['numerical_column1', 'numerical_column2']])\n",
    "\n",
    "# Min-Max scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[['numerical_column1', 'numerical_column2']] = scaler.fit_transform(df[['numerical_column1', 'numerical_column2']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical variables\n",
    "data_encoded = pd.get_dummies(data, columns=['categorical_column'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the some methods to transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-5 Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Analysis\n",
    "\n",
    "## Histograms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['numerical_column'].hist(bins=30)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Numerical Column')\n",
    "plt.show()\n",
    "\n",
    "## Bar plots for categorical data\n",
    "df['categorical_column'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bar Plot of Categorical Column')\n",
    "plt.show()\n",
    "\n",
    "# Bivariate Analysis\n",
    "\n",
    "## Scatter plots\n",
    "df.plot.scatter(x='column1', y='column2')\n",
    "plt.xlabel('Column 1')\n",
    "plt.ylabel('Column 2')\n",
    "plt.title('Scatter Plot of Column 1 vs Column 2')\n",
    "plt.show()\n",
    "\n",
    "## Box plots\n",
    "df.boxplot(column='numerical_column', by='categorical_column')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Box Plot of Numerical Column by Category')\n",
    "plt.suptitle('')\n",
    "plt.show()\n",
    "\n",
    "# Multivariate Analysis\n",
    "\n",
    "## Pair plots\n",
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(df[['column1', 'column2', 'column3']])\n",
    "plt.show()\n",
    "\n",
    "## Heatmaps for correlation\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Heatmap of Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "## Distribution Plots\n",
    "sns.distplot(df['numerical_column'])\n",
    "plt.title('Distribution Plot of Numerical Column')\n",
    "plt.show()\n",
    "\n",
    "## KDE (Kernel Density Estimate) Plots\n",
    "sns.kdeplot(df['numerical_column'])\n",
    "plt.title('KDE Plot of Numerical Column')\n",
    "plt.show()\n",
    "\n",
    "## Violin Plots\n",
    "sns.violinplot(x='categorical_column', y='numerical_column', data=df)\n",
    "plt.title('Violin Plot of Numerical Column by Category')\n",
    "plt.show()\n",
    "\n",
    "## Facet Grids\n",
    "g = sns.FacetGrid(df, col='categorical_column')\n",
    "g.map(plt.hist, 'numerical_column')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the some methods to visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-6 Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('target_column', axis=1)\n",
    "y = df['target_column']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is used to split the train data and test data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
